
<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CS180: Intro to Computer Vision and Computational Photography</title>
<link rel="StyleSheet" href="./files/style.css" type="text/css" media="all">
</head>
<body data-new-gr-c-s-check-loaded="14.1027.0" data-gr-ext-installed="">
<h1>
<center>
<h1>Fun with Filters and Frequencies!</h1>
</center>

<h1>Part 1: Fun with Filters </h1>

<h3>Part 1.1: Convolutions from Scratch!</h3>
<p class="text">

<br>
First, let's recap what a convolution is. Here, I have implemented it with four for-loops, then two for-loops. These pad with zero fill values, and output the same size as the original image. 
<br>
IMAGE my code
<br>
Let's compare these with a built-in convolution function 
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html"><code>scipy.signal.convolve2d</code></a> by convolving this selfie of me with a few different filters!
</p>
<ul>
  <li>
	9x9 box filter: IMAGE selfie with box filter  
  </li>
  <li>
	<b>D<sub>x</sub></b>: IMAGE selfie with Dx
  </li>
  <li>
	<b>D<sub>y</sub></b>: IMAGE selfie with Dy
  </li>
</ul>

<h3>Part 1.2: Finite Difference Operator</h3>

  <p class="text">
  

Now, lets look at this cameraman image. We can show the partial derivative in x and y of the  image by convolving it with <b>D_x</b> and <b> D_y </b>. This is done below.
<br>
IMAGE cameraman with Dx, cameraman with Dy
<br>
Using this, we can calculate the gradient magnitude image as follows. 
<br>
IMAGE cameraman grad mag
<br>
To turn this into an edge image, lets binarize the gradient magnitude image by picking the appropriate threshold. I considered both the outline of the cameraman and the outline of the buildings as "real edges", so I tried to balance the noise with these signals.  
<br>
IMAGE cameraman line image 1 
<br>

<h3>Part 1.3: Derivative of Gaussian (DoG) Filter</h3>

  <p class="text">
  The results with just the difference operator were rather noisy. Lets improve this with our favorite smoothing operator: the Gaussian filter <b>G</b>. By creating a blurred version of the original image by convolving with a gaussian before repeating the procedure in the previous part, we can reduces high-frequency noise before differentiation-- for a clearer final image. 
</p>  
<ul>
  <li>
    IMAGE gauss cameraman with Dx
  </li>
  <li>
  	IMAGE gauss cameraman with Dy
  </li>
  <li>
  	IMAGE gauss cameraman grad mag
  </li>
  <li>
  	IMAGE cameraman line image 1 
  </li>
</ul>
  <p class="text">
  Now we can do the same thing with a single convolution instead of two by creating a derivative of gaussian filters. Convolving the gaussian with D_x and D_y looks as follow. I used a gaussian with sigma = 1, kernel size 7, since this size was sufficient for producing the edge image. The results of this are in fact identical to those of the two convolution procedure.
  </p>
<ul>
  <li>
  	IMAGE gauss
  </li>
  <li>
    IMAGE gauss with Dx
  </li>
  <li>
  	IMAGE gauss with Dy
  </li>
</ul>

<h1> Part 2: Fun with Frequencies!</h1>

<h3>Part 2.1: Image "Sharpening"</h3>

<div align="left">
  <p class="text">

Pick your favorite blurry image and get ready to "sharpen" it! We will derive the unsharp masking technique. Remember our favorite Gaussian filter from class. This is a low pass filter that retains only the low frequencies. We can subtract the blurred version from the original image to get the high frequencies of the image. An image often looks sharper if it has stronger high frequencies. So, lets add a little bit more high frequencies to the image! Combine this into a single convolution operation which is called the unsharp mask filter. Show your result on the following image (download <a href="./files/taj.jpg">here</a>) plus other images of your choice -- </p></div><br>
<center>

<img src="./files/taj.jpg" alt="taj"><br>

</center>

  <p class="text">

Also for evaluation, pick a sharp image, blur it and then try to sharpen it again. Compare the original and the sharpened image and report your observations.

</p><h3>Part 2.2: Hybrid Images</h3>
  <center>
  <img src="./files/teaser.jpg" width="500px" alt="teaser"><br>
  <span class="text" style="font-size:12.0pt">(Look at image on right from very close, then from far away.)
</span></center>
<br>
<h3>Overview</h3>

<p class="text">The goal of this part of the assignment is to create <a href="https://en.wikipedia.org/wiki/Hybrid_image">hybrid images</a> using the approach
described in the SIGGRAPH 2006 <a href="https://web.archive.org/web/20070315210101/http://cvcl.mit.edu/hybrid/OlivaTorralb_Hybrid_Siggraph06.pdf">paper</a>
by Oliva, Torralba, and Schyns. <i>Hybrid images</i> are static images that
change in interpretation as a function of the viewing distance. The basic idea is that high frequency tends
to dominate perception when it is available, but, at a distance, only the low
frequency (smooth) part of the signal can be seen. By blending the high frequency portion of one image with the low-frequency portion of another, you get a hybrid image that leads to different interpretations at different distances.
</p>

<h3>Details</h3>

<p class="text">
<a href="https://inst.eecs.berkeley.edu/~cs180/fa24/hw/proj2/hybrid_python.zip">Here</a>, we have included two sample images (of Derek and his former cat Nutmeg) and some
starter code that can be used to load two images and align them. The alignment is important because it affects
the perceptual grouping (read the paper for details).</p>

<ol>
  <li>
    First, you'll need to get a few pairs of images that you want to make into
    hybrid images.  You can use the sample
    images for debugging, but you should use your own images in your results.  Then, you will need to write code to low-pass
    filter one image, high-pass filter the second image, and add (or average) the
    two images.  For a low-pass filter, Oliva et al. suggest using a standard 2D Gaussian filter. For a high-pass filter, they suggest using
    the impulse filter minus the Gaussian filter (which can be computed by subtracting the Gaussian-filtered image from the original).
    The <a href="http://en.wikipedia.org/wiki/Cutoff_frequency">cutoff-frequency</a> of
    each filter should be chosen with some experimentation.</li>
  <li>For your favorite result, you should also illustrate the process through frequency analysis.  Show the log magnitude of the Fourier transform of the two input images, the filtered images, and the
    hybrid image. In Python, you can compute and display the 2D Fourier transform with:
  <span style="font-size:11.0pt;font-family:Courier New">plt.imshow(np.log(np.abs(np.fft.fftshift(np.fft.fft2(gray_image)))))</span> </li>
  <li>Try creating 2-3 hybrid images (change of expression,
    morph between different objects, change over time, etc.). Show the input image and hybrid result per example. (No need to show the intermediate results as in step 2.)
    </li>
</ol>
<h3>Bells & Whistles (Extra for CS180, Mandatory for CS280A)</h3>

<p class="text">Try using color to enhance the effect.
Does it work better to use color for the high-frequency component, the
low-frequency component, or both?
</p>
<p class="text">
<h2>Multi-resolution Blending and the Oraple journey</h3>
</p><p class="text">
</p><h3>Overview</h3>
<p class="text">The goal of this part of the assignment is to blend two images seamlessly using a multi resolution blending as described in the 1983 <a href="http://persci.mit.edu/pub_pdfs/spline83.pdf">paper</a> by Burt and Adelson. An <em>image spline</em> is a smooth seam joining two image together by gently distorting them. <em>Multiresolution blending </em> computes a gentle seam between the two images seperately at each band of image frequencies, resulting in a much smoother seam.</p>  
<p align="left" class="text">We'll approach this section in two steps: 
  <ol>
    <li>creating and visualizing the Gaussian and Laplacian stacks and</li>
    <li>blending together images with the help of the completed stacks, and exploring creative outcomes </li>
  </ol>
</p>
</p><h3>Part 2.3: Gaussian and Laplacian Stacks</h3>
<p align="center"><img src="./files/stackvis.png" width="390" height="500" alt="lincoln"></p>
<p class="text"><br>
</p><center>
  <h3 align="left">Overview</h3>
  <p align="left" class="text">In this part you will implement Gaussian and Laplacian stacks, which are kind of like pyramids but without the downsampling. This will prepare you for the next step for Multi-resolution blending.</p>
  <h3 align="left">Details</h3>
</center>
<ol>
  <li>
    <div align="left"> Implement a Gaussian and a Laplacian <strong>stack</strong>. The different between a stack and a pyramid is that in each level of the pyramid the image is downsampled, so that the result gets smaller and smaller.
      In a stack the images are never downsampled so the results are all the same dimension as the original image, and can all be saved in one 3D matrix (if the original image was a grayscale image).
      To create the successive levels of the Gaussian Stack, just apply the Gaussian filter at each level, but do not subsample.
    In this way we will get a stack that behaves similarly to a pyramid that was downsampled to half its size at each level. If you would rather work with pyramids, you may implement pyramids other than stacks. However, in any case, you are <strong>NOT</strong> allowed to use built-in pyramid functions like<span style="font-family: Courier New; font-size: 11.0pt"> cv2.pyrDown() </span>or<span style="font-family: Courier New; font-size: 11.0pt"> skimage.transform.pyramid_gaussian() </span>in this project. You must implement your stacks from scratch!</div>
  </li>
  <li>Apply your Gaussian and Laplacian stacks to the Oraple and recreate the outcomes of Figure 3.42 in <a href="https://www.dropbox.com/s/bzt69u4azxyfpjo/SzeliskiBookDraft_20210828.pdf?dl=0">Szelski (Ed 2)</a> page 167, as you can see in the image above. Review the 1983 <a href="http://persci.mit.edu/pub_pdfs/spline83.pdf">paper</a> for more information.</li>
</ol>
<p class="text">
</p><p class="text">
</p><h3 align="left">Part 2.4: Multiresolution Blending (a.k.a. the oraple!)</h3>
<p align="center" class="text"><img src="./files/orple.jpg" alt="half apple half orange" width="406" height="285" align="middle">
</p><p class="text">
</p><h3>Overview</h3>
<p class="text">Review the 1983 <a href="http://persci.mit.edu/pub_pdfs/spline83.pdf">paper</a> by Burt and Adelson, if you haven't! This will provide you with the context to continue. In this part, we'll focus on actually blending two images together.</p>
<h3>Details</h3>
<p class="text"><a href="https://inst.eecs.berkeley.edu/~cs180/fa24/hw/proj2/spline.zip">Here</a>, we have included the two sample images from the paper (of an apple and an orange).</p>
<ol>
  <li> First, you'll need to get a few pairs of images that you want blend together with a vertical or horizontal seam.  You can use the sample
  images for debugging, but you should use your own images in your results. Then you will need to write some code in order to use your Gaussian and Laplacian stacks from part 2 in order to blend the images together. Since we are using stacks instead of pyramids like in the paper, the algorithm described on page 226 will not work as-is. If you try it out, you will find that you end up with a very clear seam between the apple and the orange since in the pyramid case the downsampling/blurring/upsampling hoopla ends up blurring the abrupt seam proposed in this algorithm. Instead, you should always use a mask as is proposed in the algorithm on page 230,
  and remember to create a Gaussian stack for your mask image as well as for the two input images. The Gaussian blurring of the mask in the pyramid will smooth out the transition between the two images. For the vertical or horizontal seam, your mask will simply be a step function of the same size as the original images.</li>
  <li>Now that you've made yourself an oraple (a.k.a your vertical or horizontal seam is nicely working), pick two pairs of images to blend together with an irregular mask, as is demonstrated in figure 8 in the paper.<span style="font-size:11.0pt;font-family:Courier New"></span></li>
  <li>Blend together some crazy ideas of your own!</li>
  <li>Illustrate the process by applying your  Laplacian stack and displaying it for your favorite result and the masked input images that created it.  This should look similar to Figure 10 in the paper.</li>
</ol>
<h3>Bells & Whistles (Extra for CS180, Mandatory for CS280A)</h3>
<ul>
  <li>Try using color to enhance the effect.
  </li>
</ul>


<h3>Deliverables</h3>
For this project you must turn in both your code and a project webpage as described <a href="../../hw/submitting.html">here</a> and tell us about the most important thing you learned from this project!


<br>
Also, submit the webpage URL to the class gallery via Google Form <a href="https://forms.gle/hnfh2X9kvJKns9HQ9">here</a>.

<br>
<br>

The detailed list of deliverables is released here for transparency. This is what we will check for in your website submission! However, we reserve the right to grade on a case to case basis, and we will <i>not</i> release the point distribution for each item.

<br>
<br>

Part 1: Filters and Edges
<ul>
  <li class="text"><b>Part 1.1.</b> Provide correct implementations of convolution, with numpy only. Compare with <code>scipy.signal.convolve2d</code>, and comment on runtime and how boundaries are handled.</li>
  <li class="text"><b>Part 1.2.</b> Show partial derivatives in x and y, the gradient magnitude image, and a binarized edge image. Feel free to make minor tradeoffs between finding all edges and removing all noise, but please justify your decision.</li>
  <li class="text"><b>Part 1.3.</b> Construct Gaussian filters using cv2.getGaussianKernel, build DoG filters, and visualize them. Apply both Gaussian smoothing and DoG filters to the cameraman image. Compare these results with those from the finite difference method.</li>
  <li class="text"><b>Bells and whistles.</b> Compute and visualize gradient orientations for the cameraman image (without using built-in angle functions) and explain your process.</li>
</ul>

Part 2: Applications
<ul>
  <li class="text"><b>Part 2.1.</b> Implement the unsharp mask filter. Explain how it works in relation to blur filters and high frequencies. Show the blurred, high-frequency, and sharpened versions of the Taj Mahal image, as well as another image of your choice. Demonstrate how varying the sharpening amount changes the result.</li>
  <li class="text"><b>Part 2.2.</b> Create three hybrid images (including Derek + Nutmeg and two of your own). For one hybrid, show the entire process: original and aligned images, Fourier transforms, filtered results, cutoff frequency choice, and final image. For the others, present the originals and the final hybrid.</li>
  <li class="text"><b>Part 2.2 (bells and whistles)</b> Explore, and justify your choices!</li>
  <li class="text"><b>Part 2.3 + 2.4</b> Visualize the process (show the Gaussian and Laplacian stacks for the Orange + Apple images, and recreate the outcomes of Figure 3.42 (a) through (l)). Also, include 2 additional custom blended images; one of them should have an irregular mask (not just straight lines).</li>
  <li class="text"><b>Part 2.4 (bells and whistles)</b> Explore, and justify your choices!</li>
</ul>


<h3>Scoring</h3>

<h4>CS180</h4>

<p class="text">The first part of the assignment is worth <b>30 points</b>. The following things need to be answered in the html webpage <i>along</i> with the <b>visualizations</b> mentioned in the problem statement. The distribution is as follows: </p>

<ul>
  <li class="text"> (<b>10 points</b>) Part 1.1: Convolutions from Scratch!</li>
  <li class="text"> (<b>10 points</b>) Part 1.2: Finite Difference Operator</li>
  <li class="text"> (<b>10 points</b>) Part 1.3: Derivative of Gaussian (DoG) Filter</li>
</ul>

<p class="text">The second part of the assignment is worth <b>65 points</b>, as follows: </p>

<ul>
  <li class="text"> (<b>15 points</b>) Part 2.1: Image "Sharpening"</li>
  <li class="text"> (<b>20 points</b>) Part 2.2: Hybrid Images</li>
  <li class="text"> (<b>10 points</b>) Part 2.3: Gaussian and Laplacian Stacks</li>
  <li class="text"> (<b>20 points</b>) Part 2.4: Multiresolution Blending</li>
</ul>

<p class="text"> <b>5 points</b> for clarity. This includes the quality of the visualizations, the clarity of the explanations, and the overall organization of the webpage. </p>

<p></p>
