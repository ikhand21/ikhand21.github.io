
<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CS180: Intro to Computer Vision and Computational Photography</title>
<link rel="StyleSheet" href="./files/style.css" type="text/css" media="all">
</head>
<body data-new-gr-c-s-check-loaded="14.1027.0" data-gr-ext-installed="">
<h1>
<center>
<h1>Fun with Filters and Frequencies!</h1>
</center>

<h1>Part 1: Fun with Filters </h1>

<h3>Part 1.1: Convolutions from Scratch!</h3>
<p class="text">

<br>
First, let's recap what a convolution is. Here, I have implemented it with four for-loops, then two for-loops. These pad with zero fill values, and output the same size as the original image. 
<br>
<img src="./media/code">
<br>
Let's compare these with a built-in convolution function 
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html"><code>scipy.signal.convolve2d</code></a> by convolving this selfie of me with a few different filters!
</p>
<ul>
  <li>
	9x9 box filter: <br> <img src="./media/selfie_box.png">
  </li>
  <li>
	<b>D<sub>x</sub></b>: <br> <img src="./media/selfie_dx.png">
  </li>
  <li>
	<b>D<sub>y</sub></b>: <br> <img src="./media/selfie_dy.png">
  </li>
  <li>
  FOr all of these, the scipy function was much faster, with both naive approaches being much slower. The four loop approach was much slower though. For example, the box filter took .1s with scipy's convolution function, with my four loop approach took 36s and my two loop approach took 3.5s. I also found that for the box filter, there were minute differences with order of magnitude 10^-15 between the naive convolution and scipy's. The other two were exact.  
  </li>
</ul>

<h3>Part 1.2: Finite Difference Operator</h3>

  <p class="text">
  

Now, lets look at this cameraman image. We can show the partial derivative in x and y of the  image by convolving it with <b>D_x</b> and <b> D_y </b>. This is done below, respectively.
<br>
<img src="./media/cameraman_dx.png"> <img src="./media/cameraman_dy.png">
<br>
Using this, we can calculate the gradient magnitude image as follows. 
<br>
<img src="./media/cameraman_grad_mag.png">
<br>
To turn this into an edge image, lets binarize the gradient magnitude image by picking the appropriate threshold. I considered both the outline of the cameraman and the outline of the buildings as "real edges", so I tried to balance the noise with these signals.  
<br>
<img src="./media/cameraman_grad_mag_binarized.png">
<br>

<h3>Part 1.3: Derivative of Gaussian (DoG) Filter</h3>

  <p class="text">
  The results with just the difference operator were rather noisy. Lets improve this with our favorite smoothing operator: the Gaussian filter <b>G</b>. By creating a blurred version of the original image by convolving with a gaussian before repeating the procedure in the previous part, we can reduces high-frequency noise before differentiation-- for a clearer final image. 
</p>  
<ul>
  <li>
    cameraman convolved with gaussian: <br> <img src="./media/gauss_cameraman_dx.png"> 
  </li>
  	edge image of cameraman: <br> <img src="./media/gauss_cameraman_grad_mag.png"> 
  </li>
</ul>
  <p class="text">
  Now we can do the same thing with a single convolution instead of two by creating a derivative of gaussian filters. Convolving the gaussian with D_x and D_y looks as follow. I used a gaussian with sigma = 1, kernel size 7, since this size was sufficient for producing the edge image. The results of this are in fact identical to those of the two convolution procedure.
  </p>
<ul>
  <li>
	gaussian: <br> <img src="./media/gauss.png">   
  </li>
  <li>
	DoGx: <br> <img src="./media/gauss_dx.png">   
  </li>
  <li>
	DoGy: <br> <img src="./media/gauss_dy.png">   
  </li>
</ul>

<h1> Part 2: Fun with Frequencies!</h1>

<h3>Part 2.1: Image "Sharpening"</h3>

<div align="left">
  <p class="text">

Let's derive the unsharp masking technique. As mentioned, the Gaussian filter is a low pass filter that retains only the low frequencies. We can subtract the blurred version from the original image to get the high frequencies of the image. Adding this back in emphasizes the higher frequencies more, which is interpreted as a sharper image. We can combine this into a single convolution operation which is called the unsharp mask filter. Applying this filter to a blurry image adds no extra information, but can fool one into thinking there is more detail. FOr the Taj Mahal, I used sigma = 1, which allows for minimal edges added. For my cat, I used sigma = 5 to increase the edges captured and enhanced. Here is what this looks like: 
</div>
<br>
<center>

Original image (Taj Mahal): <br> <img src="./media/taj.jpg"><br>
Sharpened image: <br> <img src="./media/sharpened_taj.jpg"><br>
Difference Between original and sharpened: <br> <img src="./media/sharpened_taj_dif.jpg"><br>

Original image (my cat): <br> <img src="./media/cat.jpg"><br>
Blurred image: <br> <img src="./media/blurred_cat.jpg"><br>
Blurred and Sharpened image: <br> <img src="./media/blurred_sharpened_cat.jpg"><br>
</center>

<h3>Part 2.2: Hybrid Images</h3>
<p class="text">
Now lets try to create <a href="https://en.wikipedia.org/wiki/Hybrid_image">hybrid images</a>. Hybrid images are static images that change in interpretation as a function of the viewing distance. High frequencies tends
to dominate our perception when it is available, but, at a distance, only the low frequency part of the signal can be seen. By blending the high frequency portion of one image with the low-frequency portion of another, you get a hybrid image that leads to different interpretations at different distances. Close up, the image should look like the first image, while far away it should look like the latter. 

<ul>
  <li>
    For a low-pass filter, I use a standard 2D Gaussian filter. For a high-pass filter, I use the impulse filter minus the Gaussian filter, computed simply as the Gaussian-filtered image subtracted from the original).
    </li>
    <li>
    The cutoff-frequency of each filter is determined by the sigma of each functions respective Gaussian filter. A high sigma for the low-pass filter results in higher blurring, or a larger range of low frequencies. Alternatively, a high sigma for the high-pass filter results in more low frequencies being removed, so the range of high frequencies is narrowed.   </li>
</ul>
<p>
  Here are my results for a few different attempts. For two trials, I tried to recreate old pictures of me as a kid, then hybridized the results. I might have succeeded too well, as the pictures happened to look very similar, making the hybrid effect harder to see. (This may also just be because I am both people).  
  <br>
  <br>
  <img src="./media/DerekPicture.jpg">, <img src="./media/nutmed.jpg">,
  <br>
	<img src="./media/out_hybrid3.jpg">,
  <br>
  <img src="./media/me_as_kid1.jpg">, <img src="./media/me_kid.jpg">
  <br>
	<img src="./media/out_hybrid2.jpg">
	<br>
  <img src="./media/me_as_kid3.jpg">, <img src="./media/me_kid4.jpg">
	<br>
	<img src="./media/out_hybrid1.jpg">,
<br>
<br>
Lets conduct frequency analysis on this last one.
</p>
<ul>  
  <li> <img src="./media/imput_image1.png"> <img src="./media/imput_image2.png">
  </li> 
  <li> 
	<img src="./media/filtered_im1.png"> <img src="./media/filtered_im2.png">
  </li>
  <li> 
	<img src="./media/hybrid_im.png">  </li>
</ul>

<h3>Multi-resolution Blending and the Oraple journey</h3>

<p class="text">The goal of this part is to blend two images seamlessly using a multi resolution blending. An image spline is a smooth seam joining two image together by gently distorting them. Multiresolution blending computes a gentle seam between the two images seperately at each band of image frequencies, resulting in a much smoother seam.</p>  
<p align="left" class="text">We'll approach this section in two steps: 
  <ol>
    <li>
    creating and visualizing the Gaussian and Laplacian stacks
    </li>
    <li>
    blending together images with the help of the completed stacks 
    </li>
  </ol>
  
<h3>Part 2.3: Gaussian and Laplacian Stacks</h3>

<p class="text">
Let's implement a Gaussian and a Laplacian <strong>stack</strong>. In a stack the images are never downsampled so the results are all the same dimension as the original image, and can all be saved in one 3D matrix (if the original image was a grayscale image). To create the successive levels of the Gaussian Stack, we apply the Gaussian filter at each level, with no subsampling.
<br>
<br>
Lets take an image of an apple and orange, to ultimately blend together. This is what their Gaussian and Laplacian stacks look like.	
</p>
<ul>
  <li>
	<img src="./media/orange_stacks.png"> 
  </li>
  <li>
	<img src="./media/apple_stacks.png"> 
  </li>
</ul>

<h3 align="left">Part 2.4: Multiresolution Blending (a.k.a. the oraple!)</h3>
<p class="text">
Now we can focus on actually blending two images together.
To do this we need to decompose each image into their Gaussian and Laplacian stacks. We also need to create a Gaussian stack for the mask. The Gaussian blurring of the mask in the pyramid will smooth out the transition between the two images. 
</p>
<ul>
  <li>  
  At each step, we combine the images, to create a resulting Laplacian stack that can then be collapsed by adding each level together (this includes the final image from the gaussian stacks) </li>
  <li> 
  Here's a simple one with a horizontal seam. <br>
	<img src="./media/leaves.png"> 
  </li>
  <li>
  Here's one with quite an irregular mask shape <br>
	  <img src="./media/penguin.png"> <img src="./media/chess.png"> <img src="./media/circle_mask.png"> <br> <img src="./media/pchess.png">
  </li>
  <li>
  And finally, the iconic oraple. <br>
	<img src="./media/orange.png"> <img src="./media/apple.png"> <br> <img src="./media/oraple.png">  </li>
</ul>

